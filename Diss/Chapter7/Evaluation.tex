% !TEX root =  ../Dissertation.tex

\chapter{Evaluation}

\section{Q-Learning Results}
link to yt video of trained agent at different stages\\
training time \\
\begin{figure}
    \centering
    \includegraphics{Preamble/BirmCrest.png}
    \caption{Graph of reward function during training of Q-Learning agent}
    \label{fig:training-reward-q}
\end{figure}
\\Simplification of action space - potentially removed best actions in some circumstances
\\Path of agent after 1000, 5000, etc. episodes
\\ \textbf{On Unsuccessful training} - reasons: \\track too complex (lots of sequential turns -> use simpler track - evidence), \\state space (Race\% not great representation of position across track -> use xz position evidence - screenshots of same race\% in different locations), \\reward function (design too specific \cite{sutton2018reinforcement} ~"encourage what you want it to complete not how to complete it" - I encountered exploitation of reward function (best action charging mt into wall) - rises problem of lots of steps before reward \cite{mnih2013playing}): multi-step rewards addresses this use checkpoint instead of race\%? also long term benefits/drawbacks - starting to drift left when right turn is next.
\\potential alternative reward function design
\\could use set of start states - allows for greater exploration through the track - states at the start line are visited a large number of times. - best action already found but eps-greedy takes random. solution? record how many visits to each state and scale epsilon value accordingly, more visits = more likely to have found best action, therefore exploit more, fewer visits = less likely to have found best action, therefore explore more. Would be problem specific - not effective in problems where say there are '2 paths' one looks bad at first but is the best and the other looks good at first but is bad at the end. \\Could also be designed using some function of episode number and step number - would need parameters to tune but could be effective - store highest step reached, explore with probability (higheststep - current step / highest step)
\subsection{State Space}

\subsection{Evaluation of Modifications to q-learning}
\subsubsection{Update Rule}
states with good values in all actions performed better than true maximum i.e. staying nearer middle of track
states with only one action that doesnt lead to termnation are seen as bad, but can sometimes be best action - ie taking tight turn: wider would mean slower but tighter leads to termination

\subsubsection{Epsilon}
progress through the track was quicker, but current Best state would be visited a lot more than all others, giving higher q value than deserved

\section{Deep Learning Results}
lots of processing required - laptop can run traditional at 60fps, but deep at less than 1fps\\
link to yt video of trained agent at different stages
\begin{figure}[hb]
    \centering
    \includegraphics{Preamble/BirmCrest.png}
    \caption{Graph of reward function during training of Rainbow Agent}
    \label{fig:trainig-reward-deep}
\end{figure}

\begin{figure}[hb]
    \centering
    \includegraphics{Preamble/BirmCrest.png}
    \caption{Figure of Agent's path at 3 stages of learning}
    \label{fig:agent-path-q}
\end{figure}

\section{Comparison to Human Players}
\subsection{Data Collection}
humans had 3 attempts at timed race (3 laps), best attempt was taken\\ timed 3 laps, best and average lap
\\ mix of abilities - new, moderate, experienced

\begin{table}[htb]
    \centering
    \begin{tabular}{l|l}
    \textbf{Player} & \textbf{Average Lap Time}\\
    \hline
    Q-Learning    &  \\
    Rainbow & \\
    Human - Experienced & \\
    Human - Intermediate & \\
    Human - Beginner &  
    \end{tabular}
    \caption{Table Comparing human and RL Performance}
    \label{tab:lap-times}
\end{table}
\section{Comparison between Approaches}
training time, data required, computational resources
generalisation capability - run trained agent on unseen track, \\input robustness - run both agents on 'busy' track, \\Scalability - when does q-table become too large
\begin{table}[hb]
    \centering
    \begin{tabular}{l|c|l}
    & \textbf{Q-Learning} & \textbf{Rainbow}\\
    \hline
    \textbf{Training Time (Episodes)}  &   &  \\
    \textbf{Training Time (Hours)}   &  &  \\
    \textbf{First lap completion(Episodes)}   &  &  \\
    \textbf{Best lap time after X Episodes}   &  &  \\
    \end{tabular}
    \caption{Table Comparing results between Deep Learning and Q-Learning}
    \label{tab:rl-comparison}
\end{table}
\begin{figure}
    \centering
    \includegraphics{Preamble/BirmCrest.png}
    \caption{Graph comparing rewards during training}
    \label{fig:q-vs-deep-rewards}
\end{figure}

hardware consideration - devices without gpu suffer a lot of slowdown from increased graphical processing - 60fps in traditional vs 0.5fps in deep
\section{Experiment: Performance on unseen track}
Best and average lap time/completion
Simple track (Luigi Circuit), complicated track (daisy circuit), perhaps custom track engineered to fool dqn - track and offroad same colour