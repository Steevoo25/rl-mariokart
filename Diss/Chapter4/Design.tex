% !TEX root =  ../Dissertation.tex
\chapter{Design}
\section{System Architecture}
explanation of architecture - python script in embedded interpreter, sends inputs through controller api, recieves state info from memory api, controlls execution with event api
\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{Figures/Q-learning-arch.png}
    \caption{System Architecture}
    \label{fig:q-learning-arch}
\end{figure}
\section{Modelling as MDP}
Q-learning does not require transition probability as it is model-free approach - learns through interaction. Great for my case as emulator allows for complete interaction with environment.
\\Episodic task - reset from savestate
\cite{watkins1992q}
\subsection{Environment}
\begin{figure}
    \centering
    \includegraphics{Figures/track-layout-mc3.png}
    \caption{Track Layout}
    \label{fig:track-layout}
\end{figure}
\begin{itemize}
    \item track - flat, various sharpness of corners, few on track obstacles, offroad borders all road
    \item kart/character combo - most widely used - allows for most comparison
    \item only agent on track - mention in future additions
    \item no item usage - mention in future additions
\end{itemize}

\subsection{Reward Function}
design process - reward function evaluation process
aims - equally reward progress and speed
encourage progress - speed, race\%
\\give bonus for completing specific actions - mt (must be fully charged before release)
\\ + Why each was chosen
\[
Reward Function
R(state) = 1
\]
\begin{figure}[ht]
    \centering
    \subfloat[Reward function of a 'well driven'\\ lap before tuning weights]{\includegraphics[width=0.5\textwidth]{Figures/Rewards-before.png}\label{fig:reward-well-driven}}
    \hfill
    \subfloat[Reward function of a 'well driven' \\lap after tuning weights]{\includegraphics[width=0.5\textwidth]{Figures/Rewards-after.png}\label{fig:reward-penalty}}
    \caption{Reward Function graphs of a 'well driven' lap}
\end{figure}
\subsection{State Space}
objectives - balance of accuracy and simplicity - large state space can lead to intractable 
\\ training time and performance
\\drift and wheelie affects steering
\\want to maintain MDP Property - all information needed to make decision 
\\ why I rounded to given value - run experiment for 3 ~1000 episodes
\begin{itemize}
    \item Speed - Speed - through design process - speed change - originally not rounded - that level of detail not required
    \item Position - race\% - why rounded to 3 dp (2dp doesnt have enough detail to show horizontal position on track) - why chose over xz (considers both x and z coords into single value - harder for humans to read but reduces state space) - 
    \item drift - 0 = not, 1 = left, 2 = right - how(from action) and why (affcts steering) direction is determined, charge = boolean, only need to know if charged or not (not how charged it is) decreases search space
    \item wheelie - true, false - affects steering and steering greatly decreases speed, before implementing this: same action in same state gave very different rewards and outcomes, leading to convergence to a local optima - acts like a toggle so unable to determine wheelie state based on current action
    \item road Type - used to determine terminal states, can be 1 or 3 on mc3, 1 = normal road, 3 = offroad
\end{itemize}
as drifts and wheelies can be held, the information need to be stored in the state to maintain the Markov Property
\begin{figure}[hb]
    \centering
    \includegraphics{Preamble/BirmCrest.png}
    \caption{Different completion values both at start line but different way across the track}
    \label{fig:completion-cross-track-difference}
\end{figure}
\subsubsection{Terminal States}
Checks speed, ground type and race\%\\
ground type: Originally set only lower speed threshold, but this was often met when going round a corner, but decreasing would lead to a lot of time spent in offroad and slow lap time, therefore changed to road type\\ also speed decays over time when in offroad was leading to spending a time step in offroad when should have terminate\\
\\speed: motivation - obstacles on track
\\lap completed (race\% greater than 2) - reset as track is same every lap, meaning for 3 lap time 
\[ S_t = \{s \in S | roadtype(s) = 3 \}\]
\\ it is for this reason that road type not used in q-table as any state with road type !=0 is terminal, therefore it would never be updated

\subsection{Action Space}
again, balance of precise inputs and limiting q-table size
\\limited set of controller inputs: A is always held (accelerate), B can only be pressed with a direction (drift), Up can only be pressed with no direction (Wheelie), 7 steering angles, sometimes cancelling a drift with small steering adjustment is useful - (steer shallow left, steer shallow right)
\\ to maintain as small q as possible. predefined 'good' inputs. Steering is mostly done with drift only small adjustments not in drift - higher speed than turning and tighter circle - small info about inside drift
\\ no turning during wheelie - greatly decreases speed 
\\ going straight - wheelie is always faster
\\starting a drift in one direction means turning in opposite direciton not possible until released - rationale for including soft turn L/R to escape drift in wrong direction
\begin{figure}
    \centering
    \includegraphics{Preamble/BirmCrest.png}
    \caption{Diagram of controller showing possible inputs}
    \label{fig:emulated-controller}
\end{figure}
\section{MDP Formalisation}

\section{Q-Learning Implementation}
explanation of what happens at every time step: pick action, execute action and observe results, update q table
\begin{figure}
    \centering
    \includegraphics{Preamble/BirmCrest.png}
    \caption{Code snippet of q-learning}
    \label{fig:q-code-snip}
\end{figure}

\section{Adjustments to Standard Q-Learning}
motivations: q learning very slow - monitoring training showed that it get stuck in local optima. Investigations showed: reward being exploited -> reward weight tuning - remove mt bonus - increased speed it enough reward in itself
\subsection{Q-Table Update Rule}
Maximisation step in update rule calculates states to be better than they are (state where every action except for one will have high value but best action not guaranteed to be taken) - take average -> allows more accurate q-value as next action is not known
\\ optimistic - unexplored actions have standard reward of 30 to not punish states where only explored actions lead to termination (30 chosen as observed average reward for a step)

\subsection{Epsilon-Greedy Policy}
Treats all stages of training the same, its expected that after some time early states will be fully explored and optimised as they are visited the most often. Once a state is fully explored, half epsilon value to encourage exploitation

\section{Data Collection}
During reward tuning reward of each component tracked at each time step
\\ during training only total reward and best epsiode's controller inputs
