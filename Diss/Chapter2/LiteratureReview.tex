% !TEX root =  ../Dissertation.tex
% how to cite github projects?
\chapter{Literature Review}
\section{Existing Work}
Due to its popularity, it is no surprise that very similar projects have been conducted. Jack Boynton gave a presentation on Deep-Reinforcement Learning \cite{JackWBoynton} where his agent used a built-in checkpoint system for the reward function, along with a series of positions, vectors and pixel data to represent the state. His work found that Hindsight Experience Replay Buffer (HER) \cite{andrychowicz2017hindsight} was effective in dealing with the sparsity of rewards, a naturally occuring problem when using checkpoints. An interesting component of the state space he designed was Cross-Track Error, which measures how far off the kart is from a pre-defined desired path. \\Ben Middleton's AI Environment \cite{BenJMiddleton} implemented an adapted version of the Rainbow Agent \cite{RainbowAgent} \cite{hessel2018rainbow} to run with Dolphin. This implementation used data from Dolphin's emulated memory, such as race completion and speed, as well as pixel data to train, and resulted in great performance in terms of lap time. This is the implementation I will be comparing mine with due to its similarities in state space, but differences in implementation, allowing insightful data analysis. 
\section{Reinforcement Learning in Games}
Games, being a popular hobby, have naturally stood out as an enticing environment to showcase advancements in Reinforcement Learning. They suit the paradigm well, due to the huge range of games available, and many tools available for interaction through code. One such tool is the Arcade Learning Environment (ALE) \cite{bellemare2013arcade}, a learning environment which provides and interface to hundreds of Atari 2600 games, spanning from simple games such as \textit{Pong} to complex platform games such as \textit{Montezuma's Revenge}. Since publication, this environment has become a go-to for many researchers due to its size and diversity. For example Mnih \textit{et al.} (2013) \cite{mnih2013playing} proposed a D-RL model known as a Deep Q Network (DQN) and compared it to two existing approaches (Contingency Awareness \cite{bellemare2012investigating} and SARSA \cite{rummery1994line}). DQN outperformed both existing approaches in all tested cases, and outperformed human players in all but one case. 
\section{Real-World Applications}
This research has various relevant real world applications. For example, E-Sports is a new and rapidly-growing industry \cite{block2021esports}, with a wide range of game genres supporting immense fanbases, the most popular being Multiplayer Online Battle Arenas (MOBAs).This is a highly complex player versus player environment, with a state space size in the magnitude of $10^{600}$. Ye \textit{et al.} (2020) \cite{ye2020mastering} beat 5 professional players with D-RL proving the potential for learning strategies from AI models in E-Sports.\\ Following a similar trajectory, RL's application to motorsport can help with race strategy, where balancing the benefits of fresh tyres with time lost in the pit lane can have a great effect on the race result. Boettinger \& Klotz (2023) \cite{boettinger2023mastering} developed a race simulation model to automate race strategy decisions, exemplifying the potential for a change in real-time decision making processes for race engineers, an integral role in any motorsport team. Additionally, Jaritz \textit{et al.} (2018) \cite{jaritz2018end} proved successful in making progress towards fully autonomous real world end-to-end race driving. Comparably, Remonda \textit{et al.} (2022) \cite{remonda2021formula} used telemetry data from a racing car simulator to learn to drive around racetracks, surpassing the baseline simulator bots. Interestingly, the generalisation capabilities of the model differed greatly depending on the training data. For example, the agent trained on a simple track with few, wider turns was not able to complete a more complex track, however the opposite was not true, demonstrating the importance of variation of training data.
%Advances in Hierarhcical \cite{duan2020hierarchical} (H-RL) and Deep RL\cite{kiran2021deep} have been applied successfully for self-driving cars, leading to the possibility of completely automated transport in a future society.
%FPS games with deep-rl - compares a trained agent with human players in deathmatch scenario - implements visual data alongside q-learning objective\cite{lample2017playing}