% !TEX root =  ../Dissertation.tex
% how to cite github projects?
\chapter{Literature Review}
\section{Existing Work}
Due to Mariokart Wii's popularity, it is no surprise that similar projects have been created. Jack Boynton (2022) developed a D-RL Mariokart Wii agent for a presentation he gave on Deep-Reinforcement Learning \cite{JackWBoynton}. This agent was  His implementation used a built-in checkpoint system for the reward function and a series of positions, vectors and pixel data for the state representation. One aspect his agent struggled with was the sparsity of rewards due the to nature of the checkpoint system. To address this he used Hindsight Experience Replay Buffer (HER) \cite{andrychowicz2017hindsight}, an extension to a standard replay buffer which generates additional transitions and goals, improving sampling efficiency. An interesting component of the state space he designed was cross-track error, which measures the kart's distance from a pre-defined desired path. This component, while effective in its specific use case, would require a new desired path to be created for each track, which would need knowledge of the track boundaries and quickest route prior to training. \\Ben Middleton's AI Environment (2022)\cite{BenJMiddleton} implemented a version of DQN called Rainbow \cite{hessel2018rainbow}, which I explain in detail later. This implementation used pixel data combined with key values from Dolphin's emulated memory as the state representation and the current speed of the kart scaled to the range $\{0,1\}$ as the reward.
\section{Reinforcement Learning in Games}
Games, being a popular hobby, have naturally stood out as an enticing environment to showcase advancements in Reinforcement Learning. They suit the paradigm well, due to the huge range of games available and existing tools available for interaction through code. One such tool is the Arcade Learning Environment (ALE) \cite{bellemare2013arcade}, a learning environment which provides an interface to hundreds of Atari 2600 games, spanning from simple games such as \textit{Pong} to complex platform games such as \textit{Montezuma's Revenge}. Since publication, this environment has become a go-to for many researchers due to its size and diversity. For example Mnih \textit{et al.} (2013) \cite{mnih2013playing} proposed a D-RL model known as a Deep Q Network (DQN) and compared it to two existing approaches (Contingency Awareness \cite{bellemare2012investigating} and SARSA \cite{rummery1994line}). DQN outperformed both existing approaches in all tested cases, and outperformed human players in all but one case. 
\section{Real-World Applications}
This research also has various relevant real world applications. For example, E-Sports is a new and rapidly-growing industry \cite{block2021esports}, with a wide range of game genres supporting immense fanbases, the most popular being Multiplayer Online Battle Arenas (MOBAs). These are highly complex player versus player environments, with an estimated state space in the magnitude of $10^{600}$. Ye \textit{et al.} (2020) \cite{ye2020mastering} beat 5 professional MOBA players with D-RL proving the potential for learning strategies from AI models in E-Sports.\\ Following a similar trajectory, RL's application to motorsport can help with race strategy, where balancing the benefits of fresh tyres with time lost in the pit lane can have a great effect on the race result. Boettinger \& Klotz (2023) \cite{boettinger2023mastering} developed a race simulation model to automate race strategy decisions, exemplifying the potential for a change in real-time decision making processes for race engineers, an integral role in any motorsport team. Comparably, Remonda \textit{et al.} (2022) \cite{remonda2021formula} used telemetry data from a racing car simulator to learn to drive around racetracks, surpassing the baseline simulator bots. Interestingly, the generalisation capabilities of the model differed greatly depending on the training data. For example, when an agent was trained on a simpler track with fewer, wider turns it was not able to complete a more complex track, however the opposite was not true.
%FPS games with deep-rl - compares a trained agent with human players in deathmatch scenario - implements visual data alongside q-learning objective\cite{lample2017playing}